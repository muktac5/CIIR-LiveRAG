from vllm import LLM, SamplingParams
from utils.json_utils import str_to_json

VALIDATOR_SYSTEM_PROMPT = """You are a helpful and capable agent who's task is to validate the response generated by the user to the given question according to a set of criteria. The user provides you with a question and a response. You also can extract some criteria from the user's question if mentioned any. Your goal is to validate the response according to the given and extracted criteria and provide feedback to the user. Your feedback should be constructive, informative, and helpful for the user to improve the response.

# Your input:
    - "question": the question the user wants to answer.
    - "information": the information the user has gathered so far. This can be empty if the user has not gathered any information yet.
    - "response": the response generated by the user.
# Your output: you need to provide a JSON object enclosed in ```json ``` that contains the following fields:
    - "extracted_criteria": a list of criteria that you extracted from only the user's question (e.g., being detailed, concise, short, long, etc.). It can be empty if you didn't extract any criteria. It should be a valid JSON list with each object in the list contains the following fields:
        - "criteria": the criteria you extracted from the user's question.
        - "criteria_explanation": an explanation of why you extracted this criteria.
        - "is_response_valid": a boolean value indicating whether the response is valid according to the extracted criteria.
        - "is_response_valid_feedback": feedback on whether the response is valid according to the extracted criteria and how it can be improved.
    - "is_groundedly_supported": a boolean value indicating whether the all parts of the response is grounded with supporting information.
    - "is_groundedly_supported_feedback": feedback on whether the response is grounded with supporting information and how it can be improved.
    - "is_correctly_answered": a boolean value indicating whether the response is correct.
    - "is_correctly_answered_feedback": feedback on whether the response is correct and how it can be improved.
Your output should be a valid JSON object enclosed in ```json ``` that contains the fields mentioned above.
"""

VALIDATOR_USER_PROMPT = """ # question: {QUESTION}
# information: {INFORMATION}
# response: {RESPONSE}
"""

def initilize_conversation():
    conversation = [
        {
            "role": "system",
            "content": VALIDATOR_SYSTEM_PROMPT
        }
    ]
    return conversation

def validate_response(question, context, response, memory, llm, execute_config):
    if 'validator' not in memory:
        memory['validator'] = initilize_conversation()
    conversation = memory['validator']
    conversation.append({
        "role": "user",
        "content": VALIDATOR_USER_PROMPT.format(QUESTION=question, INFORMATION=context, RESPONSE=response)
    })
    if execute_config['agent_model_server']:
        conversation_text = conversation
    else:
        conversation_text = llm.get_tokenizer().apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)
    sampling_parmas = SamplingParams(temperature=execute_config['temperature_agent'], top_p=execute_config['top_p'], max_tokens=execute_config['max_tokens_environment'], logprobs=1)
    response_text = llm.generate(conversation_text, sampling_parmas)[0].outputs[0].text
    response_obj = str_to_json(response_text)
    conversation.append({
        "role": "assistant",
        "content": response_text
    })
    return response_obj